{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random forest estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'%.4f'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "import regex as re\n",
    "\n",
    "\n",
    "import time\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import plot_tree\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "%precision 4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the dataset\n",
    "The dataset is divided in files depending on the CE efficiency $ \\alpha $ and the metallicity $ Z $ of the two stars. All the data are loaded into a pandas dataframe containing also the two parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data columns: \n",
      "  ['ID' 'm1ZAMS' 'm2ZAMS' 'm1rem' 'm2rem' 'm1rem+m2rem' 'delay_time' 'sma'\n",
      " 'ecc' 'kick' 'kick' 'cmu1' 'cmu2' 'v1x' 'v1y' 'v1z' 'v2x' 'v2y' 'v2z'\n",
      " 'time_SN1' 'time_SN2' 'CE' 'alpha' 'Z']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "alpha = [0.5, 1, 3, 5] # CE efficiency\n",
    "Z = [2e-4, 4e-4, 8e-4, 1.2e-3, 1.6e-3, 2e-3, 4e-3, 6e-3, 8e-3, 1.2e-2, 1.6e-2, 2e-2]  #Metallicity value\n",
    "\n",
    "data = [] #Data list \n",
    "\n",
    "sim_data = pd.DataFrame({'alpha':[], 'Z':[], 'MtotZAMS':[], 'num_mergers':[]})\n",
    "#Appending all the values in a single big dataframe\n",
    "\n",
    "index = 0\n",
    "for a in alpha:\n",
    "  for m in Z:\n",
    "    # The simulation data first row contains the MtotZAMS and the number of merger for each alpha and metallicity \n",
    "    df = pd.read_csv(f'stable_MT_vs_CE/A{a}/MTCE_BBHs_{m}.txt', sep=' ', nrows = 1, header=0) \n",
    "    sim_data.loc[index] = [a, m, df.iloc[0,0], df.iloc[0,1]]  #Adding the row to the sim_data df\n",
    "    index = index + 1 \n",
    "\n",
    "    df = pd.read_csv(f'stable_MT_vs_CE/A{a}/MTCE_BBHs_{m}.txt', header=2, sep= ' ')     # Simulation data\n",
    "    df['alpha'],df['Z'] = a, m                                 #Adding the alpha and Z parameter in the table\n",
    "    \n",
    "    data.append(df)\n",
    "\n",
    "data = pd.concat(data)\n",
    "# Cleaning the column names using regex: sub substitutes with '' all strings that matches the first regex\n",
    "# that starts from col, has any (.) char repeated any times (*) until :, and (|) from / any char, any times to the end of line $\n",
    "data.columns = [re.sub('col.*:|/.*$', '', name) for name in data.columns]\n",
    "print('Data columns: \\n ', data.columns.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2551001 BBHs evolving from common envelope, and 412581 BBHs evolving from mass transfer\n"
     ]
    }
   ],
   "source": [
    "CE_data = data.query('CE == True')\n",
    "MT_data = data.query('CE == False')\n",
    "\n",
    "\n",
    "print(f'There are {CE_data.shape[0]} BBHs evolving from common envelope, and {MT_data.shape[0]} BBHs evolving from mass transfer')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MAchine learnig part, where the features to analize are described"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_ML = data.copy()\n",
    "\n",
    "features_to_analize = ['m1ZAMS', 'm2ZAMS','m1rem','m2rem','delay_time','sma', 'ecc', 'alpha', 'Z']#,'kick','kick', 'cmu1', 'cmu2', 'v1x', 'v1y', 'v1z', 'v2x', 'v2y', 'v2z','time_SN1','time_SN2' ]\n",
    "\n",
    "x = data_to_ML[features_to_analize]\n",
    "y = data_to_ML['CE']\n",
    "\n",
    "# split data into training and test set\n",
    "# for now: 10% of data as training set\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, stratify=y, test_size=0.9, random_state=100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 42.45 s\n",
      "Training accuracy:  1.0\n",
      "Test accuracy:  0.997928183009751\n"
     ]
    }
   ],
   "source": [
    "# initialise random forest classifier\n",
    "# using n_jobs=-1 saves quite a bit of time (~80s with vs ~430s without)\n",
    "clf = RandomForestClassifier(random_state=0, n_jobs=-1)\n",
    "\n",
    "# train random forest\n",
    "# let's time this for fun\n",
    "ts = time.time()\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"Training time: %.2f s\"%(time.time()-ts))\n",
    "\n",
    "# evaluate training & test accuracy\n",
    "train_acc = clf.score(X_train, y_train)\n",
    "test_acc = clf.score(X_test, y_test)\n",
    "print(\"Training accuracy: \", train_acc)\n",
    "print(\"Test accuracy: \", test_acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importance of each parameter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# save params + importances to dataframe\n",
    "importances = pd.DataFrame(zip(clf.feature_names_in_, clf.feature_importances_),\n",
    "                           columns=[\"param\", \"gini\"])\n",
    "\n",
    "# sort importances\n",
    "#importances = importances.sort_values(by=\"gini\", ascending= False)\n",
    "\n",
    "# plot importances\n",
    "fig, ax = plt.subplots()\n",
    "sns.barplot(y='param', x='gini', data=importances, ax=ax)\n",
    "#ax.tick_params(axis='x', rotation=90) #for params on x and gini on y\n",
    "ax.set_ylabel('')\n",
    "ax.set_xlabel('Gini importance')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this part of code I'm trying to isolate the most important features by dropping one of them at a time and judging how the score changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excluding m1ZAMS: training score 100.0%, test score 99.75180187340845, time 31.0309956073761\n",
      "Excluding m2ZAMS: training score 99.99966257026975%, test score 99.42910681667531, time 30.410611867904663\n",
      "Excluding m1rem: training score 100.0%, test score 99.60674469035972, time 30.850096702575684\n",
      "Excluding m2rem: training score 100.0%, test score 99.75202682639328, time 28.861884117126465\n",
      "Excluding delay_time: training score 100.0%, test score 99.79761729798471, time 28.001099824905396\n",
      "Excluding sma: training score 100.0%, test score 99.79435547970475, time 29.31255865097046\n",
      "Excluding ecc: training score 100.0%, test score 99.78603221926618, time 28.6031653881073\n",
      "Excluding alpha: training score 100.0%, test score 99.74385353461128, time 30.822285413742065\n",
      "Excluding Z: training score 100.0%, test score 99.76421177973803, time 31.170711517333984\n"
     ]
    }
   ],
   "source": [
    "def perform_random_forest(x, y):\n",
    "  X_train, X_test, y_train, y_test = train_test_split(x, y, stratify=y, test_size=0.9, random_state=100)\n",
    "  # initialise random forest classifier\n",
    "  # using n_jobs=-1 saves quite a bit of time (~80s with vs ~430s without)\n",
    "  clf = RandomForestClassifier(random_state=0, n_jobs=-1)\n",
    "\n",
    "  # train random forest\n",
    "  # let's time this for fun\n",
    "  ts = time.time()\n",
    "  clf.fit(X_train, y_train)\n",
    "  training_time = (time.time()-ts)\n",
    "\n",
    "  # evaluate training & test accuracy\n",
    "  train_acc = clf.score(X_train, y_train)\n",
    "  test_acc = clf.score(X_test, y_test)\n",
    "  \n",
    "  return train_acc, test_acc, training_time\n",
    "\n",
    "\n",
    "data_to_ML = data[features_to_analize].copy()\n",
    "y = data['CE'].copy()\n",
    "\n",
    "for f in features_to_analize:\n",
    "  \n",
    "  train_acc, test_acc, training_time = perform_random_forest(data_to_ML.drop(columns=f), y)\n",
    "\n",
    "  print(f'Excluding {f}: training score {100*train_acc}%, test score {100*test_acc}, time {training_time}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing the same but with a linear perceptron, so also linear combinations of features can be studied (in which case a PCA could be used to restrict the data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With all fetaures: training score 94.20669406294905%, test score 94.20080551116486, time 22.67610192298889\n",
      "Excluding m1ZAMS: training score 93.37922675537615%, test score 93.39178508184685, time 34.89707899093628\n",
      "Excluding m2ZAMS: training score 64.30206515900132%, test score 64.34155401027944, time 10.058410406112671\n",
      "Excluding m1rem: training score 88.43088947336692%, test score 88.42415129788796, time 22.78215980529785\n",
      "Excluding m2rem: training score 94.2921762228223%, test score 94.29758022718438, time 33.47227954864502\n",
      "Excluding delay_time: training score 97.67978922798812%, test score 97.67767675895132, time 3.8295838832855225\n",
      "Excluding sma: training score 90.04636732313966%, test score 90.03004470263033, time 27.4143226146698\n",
      "Excluding ecc: training score 93.70149449809824%, test score 93.69466159892886, time 33.34003210067749\n",
      "Excluding alpha: training score 83.89957015970766%, test score 83.87371506932148, time 98.29842281341553\n",
      "Excluding Z: training score 93.55028105634354%, test score 93.55658553967089, time 18.734162092208862\n"
     ]
    }
   ],
   "source": [
    "def perform_perceptron(x, y):\n",
    "  X_train, X_test, y_train, y_test = train_test_split(x, y, stratify=y, test_size=0.25, random_state=100)\n",
    "  # initialise random forest classifier\n",
    "  # using n_jobs=-1 saves quite a bit of time (~80s with vs ~430s without)\n",
    "  clf = Perceptron(random_state=100, n_jobs=-1)\n",
    "\n",
    "  # train random forest\n",
    "  # let's time this for fun\n",
    "  ts = time.time()\n",
    "  clf.fit(X_train, y_train)\n",
    "  training_time = (time.time()-ts)\n",
    "\n",
    "  # evaluate training & test accuracy\n",
    "  train_acc = clf.score(X_train, y_train)\n",
    "  test_acc = clf.score(X_test, y_test)\n",
    "  \n",
    "  return train_acc, test_acc, training_time\n",
    "\n",
    "\n",
    "data_to_ML = data[features_to_analize].copy()\n",
    "y = data['CE'].copy()\n",
    "\n",
    "#With all feature\n",
    "train_acc, test_acc, training_time = perform_perceptron(data_to_ML, y)\n",
    "print(f'With all fetaures: training score {100*train_acc}%, test score {100*test_acc}, time {training_time}')\n",
    "\n",
    "for f in features_to_analize:\n",
    "  \n",
    "  train_acc, test_acc, training_time = perform_perceptron(data_to_ML.drop(columns=f), y)\n",
    "\n",
    "  print(f'Excluding {f}: training score {100*train_acc}%, test score {100*test_acc}, time {training_time}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing which feature has the biggest importance for each value of Z and alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>m1ZAMS</th>\n",
       "      <th>m2ZAMS</th>\n",
       "      <th>m1rem</th>\n",
       "      <th>m2rem</th>\n",
       "      <th>delay_time</th>\n",
       "      <th>sma</th>\n",
       "      <th>ecc</th>\n",
       "      <th>alpha</th>\n",
       "      <th>Z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.030301</td>\n",
       "      <td>0.106458</td>\n",
       "      <td>0.014277</td>\n",
       "      <td>0.038686</td>\n",
       "      <td>0.458985</td>\n",
       "      <td>0.283284</td>\n",
       "      <td>0.068009</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.062928</td>\n",
       "      <td>0.119005</td>\n",
       "      <td>0.028150</td>\n",
       "      <td>0.049686</td>\n",
       "      <td>0.465422</td>\n",
       "      <td>0.197509</td>\n",
       "      <td>0.077300</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.112489</td>\n",
       "      <td>0.089030</td>\n",
       "      <td>0.063983</td>\n",
       "      <td>0.058285</td>\n",
       "      <td>0.387026</td>\n",
       "      <td>0.219991</td>\n",
       "      <td>0.069195</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.136781</td>\n",
       "      <td>0.112891</td>\n",
       "      <td>0.079090</td>\n",
       "      <td>0.073680</td>\n",
       "      <td>0.341348</td>\n",
       "      <td>0.193489</td>\n",
       "      <td>0.062722</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.158965</td>\n",
       "      <td>0.152734</td>\n",
       "      <td>0.100209</td>\n",
       "      <td>0.088289</td>\n",
       "      <td>0.295865</td>\n",
       "      <td>0.161849</td>\n",
       "      <td>0.042088</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.186079</td>\n",
       "      <td>0.131404</td>\n",
       "      <td>0.103664</td>\n",
       "      <td>0.073514</td>\n",
       "      <td>0.236738</td>\n",
       "      <td>0.209601</td>\n",
       "      <td>0.058999</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.156309</td>\n",
       "      <td>0.212145</td>\n",
       "      <td>0.106414</td>\n",
       "      <td>0.106245</td>\n",
       "      <td>0.051723</td>\n",
       "      <td>0.113529</td>\n",
       "      <td>0.253636</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.079593</td>\n",
       "      <td>0.268987</td>\n",
       "      <td>0.158120</td>\n",
       "      <td>0.193428</td>\n",
       "      <td>0.160743</td>\n",
       "      <td>0.074132</td>\n",
       "      <td>0.064997</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.026588</td>\n",
       "      <td>0.038284</td>\n",
       "      <td>0.032318</td>\n",
       "      <td>0.033175</td>\n",
       "      <td>0.057353</td>\n",
       "      <td>0.460573</td>\n",
       "      <td>0.351709</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.011923</td>\n",
       "      <td>0.053853</td>\n",
       "      <td>0.040772</td>\n",
       "      <td>0.022344</td>\n",
       "      <td>0.005491</td>\n",
       "      <td>0.374242</td>\n",
       "      <td>0.491374</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.071501</td>\n",
       "      <td>0.122036</td>\n",
       "      <td>0.040480</td>\n",
       "      <td>0.049929</td>\n",
       "      <td>0.022405</td>\n",
       "      <td>0.336546</td>\n",
       "      <td>0.357103</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.052227</td>\n",
       "      <td>0.102209</td>\n",
       "      <td>0.128482</td>\n",
       "      <td>0.041442</td>\n",
       "      <td>0.017531</td>\n",
       "      <td>0.333074</td>\n",
       "      <td>0.325035</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.046018</td>\n",
       "      <td>0.185859</td>\n",
       "      <td>0.043182</td>\n",
       "      <td>0.093610</td>\n",
       "      <td>0.377003</td>\n",
       "      <td>0.150791</td>\n",
       "      <td>0.103538</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.109243</td>\n",
       "      <td>0.285019</td>\n",
       "      <td>0.065186</td>\n",
       "      <td>0.094457</td>\n",
       "      <td>0.132470</td>\n",
       "      <td>0.137114</td>\n",
       "      <td>0.176511</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.158971</td>\n",
       "      <td>0.257703</td>\n",
       "      <td>0.102811</td>\n",
       "      <td>0.107564</td>\n",
       "      <td>0.128660</td>\n",
       "      <td>0.132079</td>\n",
       "      <td>0.112212</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.208684</td>\n",
       "      <td>0.268244</td>\n",
       "      <td>0.163106</td>\n",
       "      <td>0.150220</td>\n",
       "      <td>0.067324</td>\n",
       "      <td>0.079219</td>\n",
       "      <td>0.063204</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.231666</td>\n",
       "      <td>0.248253</td>\n",
       "      <td>0.200391</td>\n",
       "      <td>0.136340</td>\n",
       "      <td>0.045697</td>\n",
       "      <td>0.068735</td>\n",
       "      <td>0.068917</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.257944</td>\n",
       "      <td>0.202416</td>\n",
       "      <td>0.227793</td>\n",
       "      <td>0.123701</td>\n",
       "      <td>0.044139</td>\n",
       "      <td>0.066145</td>\n",
       "      <td>0.077862</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.252625</td>\n",
       "      <td>0.150350</td>\n",
       "      <td>0.111693</td>\n",
       "      <td>0.205982</td>\n",
       "      <td>0.014632</td>\n",
       "      <td>0.046424</td>\n",
       "      <td>0.218294</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.119478</td>\n",
       "      <td>0.439544</td>\n",
       "      <td>0.071033</td>\n",
       "      <td>0.072704</td>\n",
       "      <td>0.016196</td>\n",
       "      <td>0.054489</td>\n",
       "      <td>0.226556</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.024590</td>\n",
       "      <td>0.028408</td>\n",
       "      <td>0.030543</td>\n",
       "      <td>0.022217</td>\n",
       "      <td>0.020150</td>\n",
       "      <td>0.393424</td>\n",
       "      <td>0.480667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.078716</td>\n",
       "      <td>0.079269</td>\n",
       "      <td>0.033311</td>\n",
       "      <td>0.070549</td>\n",
       "      <td>0.062410</td>\n",
       "      <td>0.398777</td>\n",
       "      <td>0.276967</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.089890</td>\n",
       "      <td>0.045882</td>\n",
       "      <td>0.091596</td>\n",
       "      <td>0.046587</td>\n",
       "      <td>0.062542</td>\n",
       "      <td>0.367514</td>\n",
       "      <td>0.295990</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.039510</td>\n",
       "      <td>0.030356</td>\n",
       "      <td>0.127955</td>\n",
       "      <td>0.049732</td>\n",
       "      <td>0.030848</td>\n",
       "      <td>0.320307</td>\n",
       "      <td>0.401292</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.217558</td>\n",
       "      <td>0.329733</td>\n",
       "      <td>0.144914</td>\n",
       "      <td>0.076945</td>\n",
       "      <td>0.064868</td>\n",
       "      <td>0.099777</td>\n",
       "      <td>0.066205</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.243683</td>\n",
       "      <td>0.256681</td>\n",
       "      <td>0.223696</td>\n",
       "      <td>0.091847</td>\n",
       "      <td>0.047697</td>\n",
       "      <td>0.067795</td>\n",
       "      <td>0.068600</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.224844</td>\n",
       "      <td>0.187739</td>\n",
       "      <td>0.309876</td>\n",
       "      <td>0.121194</td>\n",
       "      <td>0.040170</td>\n",
       "      <td>0.050433</td>\n",
       "      <td>0.065745</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.239469</td>\n",
       "      <td>0.159886</td>\n",
       "      <td>0.327304</td>\n",
       "      <td>0.134994</td>\n",
       "      <td>0.027661</td>\n",
       "      <td>0.027519</td>\n",
       "      <td>0.083167</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.246494</td>\n",
       "      <td>0.126593</td>\n",
       "      <td>0.343203</td>\n",
       "      <td>0.133075</td>\n",
       "      <td>0.030058</td>\n",
       "      <td>0.028655</td>\n",
       "      <td>0.091923</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.251357</td>\n",
       "      <td>0.091850</td>\n",
       "      <td>0.369746</td>\n",
       "      <td>0.130898</td>\n",
       "      <td>0.024920</td>\n",
       "      <td>0.025394</td>\n",
       "      <td>0.105835</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.284648</td>\n",
       "      <td>0.030403</td>\n",
       "      <td>0.332345</td>\n",
       "      <td>0.165568</td>\n",
       "      <td>0.006174</td>\n",
       "      <td>0.058835</td>\n",
       "      <td>0.122027</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.173370</td>\n",
       "      <td>0.129682</td>\n",
       "      <td>0.156781</td>\n",
       "      <td>0.316125</td>\n",
       "      <td>0.009885</td>\n",
       "      <td>0.075553</td>\n",
       "      <td>0.138604</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.078915</td>\n",
       "      <td>0.062422</td>\n",
       "      <td>0.101125</td>\n",
       "      <td>0.153095</td>\n",
       "      <td>0.055575</td>\n",
       "      <td>0.271954</td>\n",
       "      <td>0.276915</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.037691</td>\n",
       "      <td>0.051013</td>\n",
       "      <td>0.059065</td>\n",
       "      <td>0.038252</td>\n",
       "      <td>0.026613</td>\n",
       "      <td>0.369005</td>\n",
       "      <td>0.418362</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.050507</td>\n",
       "      <td>0.094291</td>\n",
       "      <td>0.175396</td>\n",
       "      <td>0.087069</td>\n",
       "      <td>0.035660</td>\n",
       "      <td>0.361855</td>\n",
       "      <td>0.195222</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.123203</td>\n",
       "      <td>0.023388</td>\n",
       "      <td>0.052657</td>\n",
       "      <td>0.094479</td>\n",
       "      <td>0.118219</td>\n",
       "      <td>0.222322</td>\n",
       "      <td>0.365731</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.252665</td>\n",
       "      <td>0.245689</td>\n",
       "      <td>0.233036</td>\n",
       "      <td>0.105865</td>\n",
       "      <td>0.038383</td>\n",
       "      <td>0.054460</td>\n",
       "      <td>0.069903</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.232790</td>\n",
       "      <td>0.188787</td>\n",
       "      <td>0.311699</td>\n",
       "      <td>0.120195</td>\n",
       "      <td>0.031751</td>\n",
       "      <td>0.045727</td>\n",
       "      <td>0.069051</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.227307</td>\n",
       "      <td>0.139255</td>\n",
       "      <td>0.354387</td>\n",
       "      <td>0.130440</td>\n",
       "      <td>0.033258</td>\n",
       "      <td>0.028416</td>\n",
       "      <td>0.086939</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.241796</td>\n",
       "      <td>0.109190</td>\n",
       "      <td>0.355205</td>\n",
       "      <td>0.137933</td>\n",
       "      <td>0.030338</td>\n",
       "      <td>0.027435</td>\n",
       "      <td>0.098103</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.252006</td>\n",
       "      <td>0.073361</td>\n",
       "      <td>0.361364</td>\n",
       "      <td>0.149988</td>\n",
       "      <td>0.027833</td>\n",
       "      <td>0.026215</td>\n",
       "      <td>0.109233</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.271175</td>\n",
       "      <td>0.042187</td>\n",
       "      <td>0.351131</td>\n",
       "      <td>0.143108</td>\n",
       "      <td>0.023383</td>\n",
       "      <td>0.046701</td>\n",
       "      <td>0.122316</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.230450</td>\n",
       "      <td>0.033663</td>\n",
       "      <td>0.218010</td>\n",
       "      <td>0.341203</td>\n",
       "      <td>0.009671</td>\n",
       "      <td>0.066109</td>\n",
       "      <td>0.100893</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.128081</td>\n",
       "      <td>0.028424</td>\n",
       "      <td>0.181671</td>\n",
       "      <td>0.375840</td>\n",
       "      <td>0.025907</td>\n",
       "      <td>0.071991</td>\n",
       "      <td>0.188086</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.168655</td>\n",
       "      <td>0.073567</td>\n",
       "      <td>0.120779</td>\n",
       "      <td>0.076452</td>\n",
       "      <td>0.079107</td>\n",
       "      <td>0.240173</td>\n",
       "      <td>0.241266</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.103363</td>\n",
       "      <td>0.084352</td>\n",
       "      <td>0.083822</td>\n",
       "      <td>0.091763</td>\n",
       "      <td>0.086255</td>\n",
       "      <td>0.367388</td>\n",
       "      <td>0.183057</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.029965</td>\n",
       "      <td>0.043434</td>\n",
       "      <td>0.155620</td>\n",
       "      <td>0.093625</td>\n",
       "      <td>0.037040</td>\n",
       "      <td>0.325835</td>\n",
       "      <td>0.314481</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.033572</td>\n",
       "      <td>0.018509</td>\n",
       "      <td>0.070600</td>\n",
       "      <td>0.281674</td>\n",
       "      <td>0.030629</td>\n",
       "      <td>0.256895</td>\n",
       "      <td>0.308121</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      m1ZAMS    m2ZAMS     m1rem     m2rem  delay_time       sma       ecc   \n",
       "0   0.030301  0.106458  0.014277  0.038686    0.458985  0.283284  0.068009  \\\n",
       "1   0.062928  0.119005  0.028150  0.049686    0.465422  0.197509  0.077300   \n",
       "2   0.112489  0.089030  0.063983  0.058285    0.387026  0.219991  0.069195   \n",
       "3   0.136781  0.112891  0.079090  0.073680    0.341348  0.193489  0.062722   \n",
       "4   0.158965  0.152734  0.100209  0.088289    0.295865  0.161849  0.042088   \n",
       "5   0.186079  0.131404  0.103664  0.073514    0.236738  0.209601  0.058999   \n",
       "6   0.156309  0.212145  0.106414  0.106245    0.051723  0.113529  0.253636   \n",
       "7   0.079593  0.268987  0.158120  0.193428    0.160743  0.074132  0.064997   \n",
       "8   0.026588  0.038284  0.032318  0.033175    0.057353  0.460573  0.351709   \n",
       "9   0.011923  0.053853  0.040772  0.022344    0.005491  0.374242  0.491374   \n",
       "10  0.071501  0.122036  0.040480  0.049929    0.022405  0.336546  0.357103   \n",
       "11  0.052227  0.102209  0.128482  0.041442    0.017531  0.333074  0.325035   \n",
       "12  0.046018  0.185859  0.043182  0.093610    0.377003  0.150791  0.103538   \n",
       "13  0.109243  0.285019  0.065186  0.094457    0.132470  0.137114  0.176511   \n",
       "14  0.158971  0.257703  0.102811  0.107564    0.128660  0.132079  0.112212   \n",
       "15  0.208684  0.268244  0.163106  0.150220    0.067324  0.079219  0.063204   \n",
       "16  0.231666  0.248253  0.200391  0.136340    0.045697  0.068735  0.068917   \n",
       "17  0.257944  0.202416  0.227793  0.123701    0.044139  0.066145  0.077862   \n",
       "18  0.252625  0.150350  0.111693  0.205982    0.014632  0.046424  0.218294   \n",
       "19  0.119478  0.439544  0.071033  0.072704    0.016196  0.054489  0.226556   \n",
       "20  0.024590  0.028408  0.030543  0.022217    0.020150  0.393424  0.480667   \n",
       "21  0.078716  0.079269  0.033311  0.070549    0.062410  0.398777  0.276967   \n",
       "22  0.089890  0.045882  0.091596  0.046587    0.062542  0.367514  0.295990   \n",
       "23  0.039510  0.030356  0.127955  0.049732    0.030848  0.320307  0.401292   \n",
       "24  0.217558  0.329733  0.144914  0.076945    0.064868  0.099777  0.066205   \n",
       "25  0.243683  0.256681  0.223696  0.091847    0.047697  0.067795  0.068600   \n",
       "26  0.224844  0.187739  0.309876  0.121194    0.040170  0.050433  0.065745   \n",
       "27  0.239469  0.159886  0.327304  0.134994    0.027661  0.027519  0.083167   \n",
       "28  0.246494  0.126593  0.343203  0.133075    0.030058  0.028655  0.091923   \n",
       "29  0.251357  0.091850  0.369746  0.130898    0.024920  0.025394  0.105835   \n",
       "30  0.284648  0.030403  0.332345  0.165568    0.006174  0.058835  0.122027   \n",
       "31  0.173370  0.129682  0.156781  0.316125    0.009885  0.075553  0.138604   \n",
       "32  0.078915  0.062422  0.101125  0.153095    0.055575  0.271954  0.276915   \n",
       "33  0.037691  0.051013  0.059065  0.038252    0.026613  0.369005  0.418362   \n",
       "34  0.050507  0.094291  0.175396  0.087069    0.035660  0.361855  0.195222   \n",
       "35  0.123203  0.023388  0.052657  0.094479    0.118219  0.222322  0.365731   \n",
       "36  0.252665  0.245689  0.233036  0.105865    0.038383  0.054460  0.069903   \n",
       "37  0.232790  0.188787  0.311699  0.120195    0.031751  0.045727  0.069051   \n",
       "38  0.227307  0.139255  0.354387  0.130440    0.033258  0.028416  0.086939   \n",
       "39  0.241796  0.109190  0.355205  0.137933    0.030338  0.027435  0.098103   \n",
       "40  0.252006  0.073361  0.361364  0.149988    0.027833  0.026215  0.109233   \n",
       "41  0.271175  0.042187  0.351131  0.143108    0.023383  0.046701  0.122316   \n",
       "42  0.230450  0.033663  0.218010  0.341203    0.009671  0.066109  0.100893   \n",
       "43  0.128081  0.028424  0.181671  0.375840    0.025907  0.071991  0.188086   \n",
       "44  0.168655  0.073567  0.120779  0.076452    0.079107  0.240173  0.241266   \n",
       "45  0.103363  0.084352  0.083822  0.091763    0.086255  0.367388  0.183057   \n",
       "46  0.029965  0.043434  0.155620  0.093625    0.037040  0.325835  0.314481   \n",
       "47  0.033572  0.018509  0.070600  0.281674    0.030629  0.256895  0.308121   \n",
       "\n",
       "    alpha       Z  \n",
       "0     0.5  0.0002  \n",
       "1     0.5  0.0004  \n",
       "2     0.5  0.0008  \n",
       "3     0.5  0.0012  \n",
       "4     0.5  0.0016  \n",
       "5     0.5  0.0020  \n",
       "6     0.5  0.0040  \n",
       "7     0.5  0.0060  \n",
       "8     0.5  0.0080  \n",
       "9     0.5  0.0120  \n",
       "10    0.5  0.0160  \n",
       "11    0.5  0.0200  \n",
       "12    1.0  0.0002  \n",
       "13    1.0  0.0004  \n",
       "14    1.0  0.0008  \n",
       "15    1.0  0.0012  \n",
       "16    1.0  0.0016  \n",
       "17    1.0  0.0020  \n",
       "18    1.0  0.0040  \n",
       "19    1.0  0.0060  \n",
       "20    1.0  0.0080  \n",
       "21    1.0  0.0120  \n",
       "22    1.0  0.0160  \n",
       "23    1.0  0.0200  \n",
       "24    3.0  0.0002  \n",
       "25    3.0  0.0004  \n",
       "26    3.0  0.0008  \n",
       "27    3.0  0.0012  \n",
       "28    3.0  0.0016  \n",
       "29    3.0  0.0020  \n",
       "30    3.0  0.0040  \n",
       "31    3.0  0.0060  \n",
       "32    3.0  0.0080  \n",
       "33    3.0  0.0120  \n",
       "34    3.0  0.0160  \n",
       "35    3.0  0.0200  \n",
       "36    5.0  0.0002  \n",
       "37    5.0  0.0004  \n",
       "38    5.0  0.0008  \n",
       "39    5.0  0.0012  \n",
       "40    5.0  0.0016  \n",
       "41    5.0  0.0020  \n",
       "42    5.0  0.0040  \n",
       "43    5.0  0.0060  \n",
       "44    5.0  0.0080  \n",
       "45    5.0  0.0120  \n",
       "46    5.0  0.0160  \n",
       "47    5.0  0.0200  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#For all values of metallicity and alpha\n",
    "\n",
    "clf = RandomForestClassifier(random_state=0, n_jobs=-1)\n",
    "\n",
    "importance_list = []\n",
    "\n",
    "for a in alpha:\n",
    "  for z in Z:\n",
    "   \n",
    "    data_to_ML = data.query(f'alpha == {a} and Z == {z}')\n",
    "    x = data_to_ML[features_to_analize]\n",
    "    y = data_to_ML['CE']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.30, random_state=100)\n",
    "    clf.fit(X_train, y_train)\n",
    "    importances = clf.feature_importances_\n",
    "    importances[-1] = z \n",
    "    importances[-2] = a \n",
    "    importance_list.append(importances)\n",
    "\n",
    "importance = pd.DataFrame(importance_list, columns=features_to_analize)\n",
    "\n",
    "importance    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'importance' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mimportance\u001b[49m\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mZ\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124malpha\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39maggregate(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m sns\u001b[38;5;241m.\u001b[39mheatmap(df)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'importance' is not defined"
     ]
    }
   ],
   "source": [
    "df = importance.drop(columns='Z')\n",
    "df = df.groupby('alpha').aggregate('sum')\n",
    "sns.heatmap(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'importance' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mimportance\u001b[49m\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124malpha\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mZ\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39maggregate(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m sns\u001b[38;5;241m.\u001b[39mheatmap(df)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'importance' is not defined"
     ]
    }
   ],
   "source": [
    "df = importance.drop(columns='alpha')\n",
    "df = df.groupby('Z').aggregate('sum')\n",
    "sns.heatmap(df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
