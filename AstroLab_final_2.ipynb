{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F6OC7XZYG53y"
   },
   "source": [
    "# Binary star evolution and binary black holes\n",
    "\n",
    "Giacomo Menegatti, Dario Puggioni, Laura Schulze, Savina Tsichli"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NdzvZVN7G535"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "\n",
    "The discovery of gravitational waves in 2015 marked a significant breakthrough, offering new insights into the physics and astrophysics of compact celestial objects. Since then, numerous gravitational wave events have been observed, predominantly attributed to the merging of two black holes (BHs). Finding out more about these systems is crucial, as they provide us with information about the evolution and formation of **Binary Black Holes (BBHs)** , which usually originate from a binary star system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Throughout this system’s lifespan, a closely orbiting pair of massive stars experiences a series of complex physical transformations that change its final state. These include the exchange of mass between the two stars, which can occur through:\n",
    "- **Stable Mass Transfer (SMT)** , or\n",
    "- **Common Envelope (CE)**\n",
    "\n",
    "These interactions significantly affect the eventual masses and orbital characteristics of the resulting binary black hole system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially, the two stars are both on the main sequence (MS). When the most massive one leaves the MS, its radius starts inflating and can grow dramatically. \n",
    "\n",
    "**Stable Mass Transfers** happen mainly due to Roche-lobe filling, but stellar winds may also be the cause; if the massive star’s radius equals the Roche lobe, the system starts the SMT episode. Roche-lobes are regions surrounding the stars that are determined by the gravitational interaction between two stars; their shape resembles a teardrop with the apex pointing towards the companion star. When a star expands beyond its Roche-lobe, material can start flowing towards its companion star due to gravitational forces causing the mass transfer. \n",
    "\n",
    "<div>\n",
    "<center><img src=\"Picture4.png\" width=\"550\"/></center>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some mass is lost from the system while some is transferred to the companion star. \n",
    "\n",
    "However, if both stars fill their Roche-lobe (when the secondary star also leaves the MS) or Mass Transfer is dynamically unstable, then the binary system is expected to merge via **Common Envelope (CM)**. Their outer envelopes cease to rotate synchronously with their cores and this results in both stellar cores being enveloped by the same non-rotating outer layer. Consequently, they begin spiraling inward due to the drag exerted by the envelope. As this happens, a portion of their orbital energy is dissipated causing the heating of the envelope and making it less tightly bound; this may lead to the ejection of the envelope. This way the binary survives, and it is composed of two naked stellar cores; if the two cores evolve into a BBH system, it will have a short semi-major axis (much smaller than the sum of the maximum radii of its progenitor stars).\n",
    "\n",
    "<div>\n",
    "<center><img src=\"Picture1.png\" width=\"550\"/></center>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most common way to describe a Common Envelope is the **α** formalism; the energy required to expel the envelope originates solely from the loss of orbital energy during the spiral in of the two cores. α is a dimensionless parameter that measures this loss of energy; the larger the α, the more easily ejected the envelope is and vice versa.\n",
    "In this notebook, the simulated systems used can have values of α>1. This happens because this simple formalism poorly describes the physics of Common Envelope as these interactions are much more complex. A comprehensive treatment of CE must consider various additional factors beyond just the orbital energy of the cores and the binding energy of the envelope.\n",
    "\n",
    "<div>\n",
    "<center><img src=\"Picture3.png\" width='300'/></center>    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this reason, in this project the characteristics of these systems will be analyzed. \n",
    "This project aims to differentiate binary black hole systems that evolved via stable mass transfer from those who evolved via common envelop by investigating simulation data. In each dataset of the simulated data, systems which underwent common envelope are marked with \"True\" and systems which underwent stable mass transfer are marked with \"False\" in the \"CE\" parameter. The data are analyzed to identify the features with the highest impact on the evolution of the binary system regarding mass transfer and common envelope. For this purpose, the biserial correlation as well as the parameter importances in a machine learning model are investigated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "L9g3VwUgG539"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'regex'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-a32ddaa40a84>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_style\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"darkgrid\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mregex\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensemble\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'regex'"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "import regex as re\n",
    "import time\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.inspection import permutation_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uI-EavVDG54A"
   },
   "source": [
    "## Loading the dataset\n",
    "The dataset is divided in files depending on the CE efficiency $ \\alpha $ and the metallicity $ Z $ of the two stars. All the data are loaded into a pandas dataframe containing also the two parameters. The following table contains the parameter names, units and descriptions.\n",
    "\n",
    "\n",
    "| Column name | Unit | Parameter description |\n",
    "|------|------|------|\n",
    "| m1ZAMS | $M_\\odot$ | zero-age main sequence mass of the primary member of the binary system |\n",
    "| m2ZAMS | $M_\\odot$ | zero-age main sequence mass of the secondary member of the binary system |\n",
    "| m1rem | $M_\\odot$ | mass of the black hole that forms from the primary member |\n",
    "| m2rem | $M_\\odot$ | mass of the black hole that forms from the secondary member |\n",
    "| m1rem+m2rem | $M_\\odot$ |  sum of the masses of the two black holes |\n",
    "| delay_time | Myr | delay time |\n",
    "| sma | $R_\\odot$ | semi-major axis of the binary system at the formation of the second-born black hole |\n",
    "| ecc | - | orbital eccentricity of the binary system at the formation of the second-born black hole | kick_1 | km/a | magnitude of the supernova kick associated with the primary member |\n",
    "| kick_1 | km/s | magnitude of the supernova kick associated with the primary member |\n",
    "| kick_2 | km/s | magnitude of the supernova kick associated with the secondary member |\n",
    "| cmu1 | - | cosine of the tilt angle between the orbital angular momentum of the binary system before and after the supernova explosion of the primary member |\n",
    "| cmu2 | - | cosine of the tilt angle between the orbital angular momentum of the binary system before and after the supernova explosion of the secondary member |\n",
    "| time_SN1 | Myr | time at which the primary component undergoes a supernova |\n",
    "| time_SN2 | Myr | time at which the secondary component undergoes a supernova |\n",
    "| CE | - | Boolean variable indicating whether the binary system undergoes a common envelope (True) |\n",
    "| alpha | - | efficiency parameter of common envelope |\n",
    "| Z | - | stellar metallicity at which the black holes were produced |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xfd9JZnqG54B"
   },
   "outputs": [],
   "source": [
    "alpha = [0.5, 1, 3, 5] # CE efficiency\n",
    "Z = [2e-4, 4e-4, 8e-4, 1.2e-3, 1.6e-3, 2e-3, 4e-3, 6e-3, 8e-3, 1.2e-2, 1.6e-2, 2e-2]  #Metallicity value\n",
    "\n",
    "data = [] #Data list\n",
    "\n",
    "sim_data = pd.DataFrame({'alpha':[], 'Z':[], 'MtotZAMS':[], 'num_mergers':[]})\n",
    "#Appending all the values in a single big dataframe\n",
    "\n",
    "index = 0\n",
    "for a in alpha:\n",
    "    for m in Z:\n",
    "        # The simulation data first row contains the MtotZAMS and the number of merger for each alpha and metallicity\n",
    "        df = pd.read_csv(f'stable_MT_vs_CE/A{a}/MTCE_BBHs_{m}.txt', sep=' ', nrows = 1, header=0)\n",
    "        sim_data.loc[index] = [a, m, df.iloc[0,0], df.iloc[0,1]]  #Adding the row to the sim_data df\n",
    "        index = index + 1\n",
    "\n",
    "        df = pd.read_csv(f'stable_MT_vs_CE/A{a}/MTCE_BBHs_{m}.txt', header=2, sep= ' ')     # Simulation data\n",
    "        df['alpha'],df['Z'] = a, m                                 #Adding the alpha and Z paramtere in the table\n",
    "\n",
    "        data.append(df)\n",
    "\n",
    "data = pd.concat(data)\n",
    "data.columns = [re.sub('col.*:|/.*$', '', name) for name in data.columns]\n",
    "\n",
    "# separate the kick columns\n",
    "data.columns.values[9] = 'kick_1'\n",
    "data.columns.values[10] = 'kick_2'\n",
    "\n",
    "# switch vx and kick columns\n",
    "data[\"kick_1\"], data[\"v1x\"] =  data[\"v1x\"], data[\"kick_1\"]\n",
    "data[\"kick_2\"], data[\"v2x\"] =  data[\"v2x\"], data[\"kick_2\"]\n",
    "\n",
    "# Drop the ID column\n",
    "data = data.drop(columns=[\"ID\",\"v1x\",\"v1y\",\"v1z\",\"v2x\",\"v2y\",\"v2z\"]) #We drop velocities due to \n",
    "#instructions given by prof. Iorio\n",
    "\n",
    "print('Data columns: \\n ', data.columns.values)\n",
    "data.reset_index(drop=True,inplace=True)\n",
    "data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tCBUs7OEG54C"
   },
   "outputs": [],
   "source": [
    "N_data = data.shape[0]\n",
    "\n",
    "CE_data = data.query('CE == True').shape[0]\n",
    "MT_data = data.query('CE == False').shape[0]\n",
    "print(f'There are {CE_data} ({CE_data/N_data * 100:.2f}%) BBHs evolving from common envelope, and {MT_data} ({MT_data/N_data * 100:.2f}%) BBHs evolving from mass transfer, for a total of {N_data} BBHs')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing the correlation between parameters\n",
    "Before assessing the importance of features, we examinate the correlation between features themselves. It is anticipated that stronger correlations between features might affect how their importance is evaluated.\n",
    "\n",
    "Here, we examine the Spearman correlation between the features. For visualization purposes, only the upper diagonal part of the matrices is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Btlz3rrMG54H"
   },
   "outputs": [],
   "source": [
    "data_corr_CE = data.query('CE==True').drop(columns='CE')\n",
    "data_corr_MT = data.query('CE==False').drop(columns='CE')\n",
    "\n",
    "fig,axs=plt.subplots(1,2,figsize=(15,5))\n",
    "\n",
    "corr_matrix_CE = data_corr_CE.corr(method='spearman')\n",
    "mask = np.triu(np.ones_like(corr_matrix_CE, dtype=bool))\n",
    "sns.heatmap((corr_matrix_CE), mask=mask, cmap='jet', linecolor='w',ax=axs[0])\n",
    "axs[0].set_title('Spearman correlation of the features, Common Envelope')\n",
    "\n",
    "corr_matrix_MT = data_corr_MT.corr(method='spearman')\n",
    "mask = np.triu(np.ones_like(corr_matrix_MT, dtype=bool))\n",
    "sns.heatmap((corr_matrix_MT), mask=mask, cmap='jet', linecolor='w',ax=axs[1])\n",
    "axs[1].set_title('Spearman correlation of the features, Mass Transfer')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By looking at the heatmaps above we notice that the most correlated parameters are the masses and the time of supernova explosions, as well as the kick velocities and the eccentricities. More specifically, for the Common Envelope we notice that the features of eccentricity and kick velocities are strongly correlated with the cosine of the tilt angle, while this does not seem to happen for the Mass Transfer case. Finally, alpha is a parameter that only affects Common Envelope, therefore it is not correlated to any other parameters in the case of Mass Transfer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PvTEr8OXG54D"
   },
   "source": [
    "## Investigating biserial correlation of the parameters\n",
    "\n",
    "In this section the point biserial correlation is calculated for each feature with respect to the binary parameter \"CE\". In the following table the values can range from -1 to 1 and indicate the grade of correlation between data and a binary condition.\n",
    "\n",
    "We expect that the more strongly correlated data will be the most useful in our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qHO5PqViG54D"
   },
   "outputs": [],
   "source": [
    "numeric_columns = data.drop(columns=\"CE\")\n",
    "correlation_dict = {}\n",
    "for column in numeric_columns:\n",
    "    correlation = data[column].corr(data['CE'])\n",
    "    correlation_dict[column] = correlation\n",
    "correlation_df=pd.DataFrame(correlation_dict,index=['Correlation']).T\n",
    "correlation_df.sort_values(by='Correlation',key=abs,ascending=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LbUgJOp0G54F"
   },
   "source": [
    "Histograms of the data for each features are shown, plotting them with a different color for the binary condition \"CE\". They are normalized to better visualize two datasets that had different amounts of data in the same plot. Furthermore, for some of these histograms a logarithmic scale is used for the counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PXTwhwQNG54F"
   },
   "outputs": [],
   "source": [
    "# Function to plot the parameters (only for continue distributions)\n",
    "def plot_continue(ax, feature, uom='', bins = 50, hist_range=None, log=False):\n",
    "    if hist_range == None:\n",
    "        hist_range = (data[feature].min(), data[feature].max())\n",
    "    bin_width = (hist_range[1]-hist_range[0])/bins\n",
    "    data[data.CE == True][feature].hist(bins = bins, range = hist_range, alpha=0.5, label='Common Envelope', ax=ax,density=True, log=log)\n",
    "    data[data.CE == False][feature].hist(bins = bins, range = hist_range, alpha=0.5, label='Mass Transfer', ax=ax,density=True, log=log)\n",
    "    ax.legend(loc='best')\n",
    "    ax.set_xlabel(f'{feature} [{uom}]')\n",
    "    ax.set_ylabel(f'Relative counts/({bin_width:.3f} {uom})')\n",
    "    ax.set_title(f'{feature}, correlation = {correlation_dict[feature]:.4f}')\n",
    "    return ax\n",
    "\n",
    "def plot_discrete(ax, feature):\n",
    "    # Function to plot a discrete variable\n",
    "\n",
    "    labels_CE, counts_CE = np.unique(data[data['CE'] == True][feature], return_counts= True)\n",
    "    # Making the labels strings so they are equally spaced on the plot\n",
    "    labels_CE = [str(x) for x in labels_CE]\n",
    "    ax.set_xticklabels(labels_CE, rotation=45)\n",
    "    labels_MT, counts_MT = np.unique(data[data['CE'] == False][feature], return_counts= True)\n",
    "    labels_MT = [str(x) for x in labels_CE]\n",
    "    ax.bar(labels_CE, counts_CE/CE_data, alpha=0.5,label='Common envelope')\n",
    "    ax.bar(labels_MT, counts_MT/MT_data, alpha=0.5,label='Mass transfer')\n",
    "    ax.set_ylabel(f'Relative counts')\n",
    "    ax.set_xlabel(f'{feature}')\n",
    "    ax.set_title(f'{feature}, correlation = {correlation_dict[feature]:.4f}')\n",
    "    ax.legend(loc='best')\n",
    "    return ax\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(nrows=8, ncols=2, figsize=(12, 40))\n",
    "\n",
    "ax = plot_continue(axs[0,0], 'm1ZAMS', '$M_\\odot$')\n",
    "ax = plot_continue(axs[0,1], 'm2ZAMS', '$M_\\odot$')\n",
    "ax = plot_continue(axs[1,0], 'm1rem', '$M_\\odot$')\n",
    "ax = plot_continue(axs[1,1], 'm2rem', '$M_\\odot$')\n",
    "ax = plot_continue(axs[2,0], 'm1rem+m2rem', '$M_\\odot$')\n",
    "ax = plot_continue(axs[2,1], 'delay_time', 'Myr')\n",
    "ax = plot_continue(axs[3,0], 'sma', '$R_\\odot$', log=True)\n",
    "ax = plot_continue(axs[3,1], 'ecc', '')\n",
    "ax = plot_continue(axs[4,0], 'cmu1', '', log=True)\n",
    "ax = plot_continue(axs[4,1], 'cmu2', '', log=True)\n",
    "ax = plot_continue(axs[5,0], 'kick_1', 'km/s', hist_range=(-1, 20))\n",
    "ax = plot_continue(axs[5,1], 'kick_2', 'km/s', hist_range=(-1,20))\n",
    "#ax = plot_continue(axs[6,0], 'v1x', 'km/s', log=True)\n",
    "#ax = plot_continue(axs[6,1], 'v2x', 'km/s', log=True)\n",
    "#ax = plot_continue(axs[7,0], 'v1y', 'km/s',  log=True)\n",
    "#ax = plot_continue(axs[7,1], 'v2y', 'km/s',  log=True)\n",
    "#ax = plot_continue(axs[8,0], 'v1z', 'km/s', log=True)\n",
    "#ax = plot_continue(axs[8,1], 'v2z', 'km/s', log=True)\n",
    "ax = plot_continue(axs[6,0], 'time_SN1', 'Myr')\n",
    "ax = plot_continue(axs[6,1], 'time_SN2', 'Myr')\n",
    "\n",
    "#Plotting alpha and Z\n",
    "\n",
    "ax = plot_discrete(axs[7,0], 'alpha')\n",
    "ax = plot_discrete(axs[7,1], 'Z')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XSSqPh4HG54H"
   },
   "source": [
    "## Investigating parameter importance through Machine Learning\n",
    "In the following section, the importance of the parameters is examined further by evaluating a machine learning model.\n",
    "First, a Random Forest model is trained on the simulation data to predict whether a system evolves via common envelope or not. Then, the impact of the different parameters on the model and its prediction accuracy is investigated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A4LHDTtyG54I"
   },
   "source": [
    "### Training the Random Forest Model\n",
    "The Random Forest was implemented using the `RandomForestClassifier` from the sklearn.ensemble module. As per default, the Random Forest consists of 100 decision trees. \n",
    "\n",
    "We implement this algorithm twice, once with the original data and once adding a simulated feature using Gaussian distribution; that does not add information to the data, but it helps setting a lower limit to the importances: if a feature has a similar importance to the random simulated one, it means it is not important. This has been done because the Gini importance is biased towards high cardinality data.\n",
    "\n",
    "Random Forest algorithm is trained using a 10%-subset of the simulation data, and the prediction accuracy is tested using the remaining 90% of the data. A test of approx. 99.8% accuracy is achieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jvmo8nJoG54I"
   },
   "outputs": [],
   "source": [
    "# prepare data X and labels y\n",
    "X = data.drop(columns='CE')\n",
    "y = data['CE']\n",
    "\n",
    "# split data into training and test set (10% to 90%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.9, random_state=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RX7sS9GCG54J"
   },
   "outputs": [],
   "source": [
    "# initialise random forest classifier\n",
    "# using n_jobs=-1 to run parallel jobs using all processors\n",
    "clf = RandomForestClassifier(random_state=0, n_jobs=-1)\n",
    "\n",
    "# train random forest\n",
    "# time the process\n",
    "ts = time.time()\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"Training time: %.2f s\"%(time.time()-ts))\n",
    "\n",
    "# evaluate training & test accuracy\n",
    "train_acc = clf.score(X_train, y_train)\n",
    "test_acc = clf.score(X_test, y_test)\n",
    "print(\"Training accuracy: \", train_acc)\n",
    "print(\"Test accuracy: \", test_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1_4TWMLbG54L"
   },
   "outputs": [],
   "source": [
    "X_noise = X.copy()\n",
    "X_noise['noise'] = np.random.normal(0, 1, size = N_data)\n",
    "\n",
    "# split data into training and test set (10% to 90%)\n",
    "X_train_noisy, X_test_noisy, y_train_n, y_test_n = train_test_split(X_noise, y, stratify=y, test_size=0.9, random_state=100)\n",
    "\n",
    "# initialise random forest classifier\n",
    "# using n_jobs=-1 to run parallel jobs using all processors\n",
    "clf_noisy = RandomForestClassifier(random_state=0, n_jobs=-1)\n",
    "\n",
    "# train random forest\n",
    "# time the process\n",
    "ts = time.time()\n",
    "clf_noisy.fit(X_train_noisy, y_train)\n",
    "print(\"Training time: %.2f s\"%(time.time()-ts))\n",
    "\n",
    "# evaluate training & test accuracy\n",
    "train_acc_n = clf_noisy.score(X_train_noisy, y_train)\n",
    "test_acc_n = clf_noisy.score(X_test_noisy, y_test)\n",
    "print(\"Training accuracy: \", train_acc_n)\n",
    "print(\"Test accuracy: \", test_acc_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZHNegeu9G54J"
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "def show_confusion_matrix(validations, predictions,axes,perc=False):\n",
    "    LABELS = ['MT','CE']\n",
    "    matrix = metrics.confusion_matrix(validations, predictions)\n",
    "    #plt.figure(figsize=(6, 5))\n",
    "    M = matrix\n",
    "    f = 'd'\n",
    "    cmap=\"GnBu\"\n",
    "    if perc:\n",
    "        M=M/np.sum(M)\n",
    "        f='.2%'\n",
    "        cmap='Blues'\n",
    "    sns.heatmap(M, xticklabels=LABELS, yticklabels=LABELS, annot=True, fmt=f, linecolor='white', linewidths=1, cmap=cmap,ax=axes)\n",
    "    #plt.title('Confusion Matrix')\n",
    "    axes.set_ylabel('True Label')\n",
    "    axes.set_xlabel('Predicted Label')\n",
    "    #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(1,2,figsize=(16,6))\n",
    "\n",
    "show_confusion_matrix(y_test, clf.predict(X_test), ax[0], perc=True)\n",
    "show_confusion_matrix(y_test_n, clf_noisy.predict(X_test_noisy), ax[1], perc=True)\n",
    "ax[0].set_title('Confusion matrix with original data')\n",
    "ax[1].set_title('Confusion matrix with additional noise')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen from the Confusion matrices, adding random noise does not seem to affect the performances of the algorithms. For that reason, it will not be used in the following analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gYNHN7neG54K"
   },
   "source": [
    "### Analyzing feature importance\n",
    "After training a model that predicts with a satisfactory accuracy, the importance of the features is evaluated using two different metrics; the Gini importance and the Permutation importance.\n",
    "The Gini importance is based on the decrease in impurity, whereas the permutation importance is based on the mean decrease in accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ekn0mMsKG54K"
   },
   "source": [
    "#### Gini importance\n",
    "The Gini importance of a feature is estimated through the weighted impurity decrease at the nodes where the feature is used to split samples in the decision trees. In scikit-learn, the Gini importances are available through the `feature_importances_`-attribute of the `RandomForestClassifier`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "39WyPGakG54K"
   },
   "outputs": [],
   "source": [
    "# save params + importances to dataframe\n",
    "importances = pd.DataFrame(zip(clf.feature_names_in_, clf.feature_importances_),columns=[\"param\", \"gini\"])\n",
    "importances_noisy = pd.DataFrame(zip(clf_noisy.feature_names_in_, clf_noisy.feature_importances_),columns=[\"param\", \"gini\"])\n",
    "\n",
    "# plot Gini importances\n",
    "fig, ax = plt.subplots(1,2,figsize=(17,7))\n",
    "sns.barplot(y='param', x='gini', data=importances, ax=ax[0], hue='param')\n",
    "ax[0].set_ylabel('')\n",
    "ax[0].set_xlabel('Gini importance')\n",
    "ax[0].set_title('Gini importances with original data')\n",
    "\n",
    "\n",
    "clrs = ['r' if (param == 'noise') else 'b' for param in clf_noisy.feature_names_in_]\n",
    "sns.barplot(y='param', x='gini', data=importances_noisy, ax=ax[1], hue='param')\n",
    "ax[1].set_ylabel('')\n",
    "ax[1].set_xlabel('Gini importance with noisy channels')\n",
    "ax[1].set_title('Gini importances with additional noise')\n",
    "forest_importances.plot.bar(xerr=std, ax=ax[1])\n",
    "\n",
    "plt.tight_layout(h_pad=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the importance of the features are nearly unchanged with or without the addition of an extra column of noise. You can notice that cmu2 has an importance even lower than the noise, so it is not important at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VYAP0U0aG54M"
   },
   "source": [
    "#### Permutation Feature Importance\n",
    "The permutation importance of a feature describes the mean decrease in accuracy when the values of that feature are randomly permutated, and is computed using out-of-bag samples. Here, the permutation feature importances were calculated with the test data set using the `sklearn.inspection.permutation_importance` method.\n",
    "\n",
    "As the computation of the permutation importance using the entire test set takes a lot of time, a subset corresponding to 10% of the total data was chosen from the test set. Furthermore, the number of repetitions `n_repeats` was set to 5 to limit the computation time as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7KBEw7ZCG54M"
   },
   "outputs": [],
   "source": [
    "# compute permutation feature importances\n",
    "# warning: takes a lot of time to compute!\n",
    "ts = time.time()\n",
    "result = permutation_importance(clf, X_test[::10], y_test[::10], n_repeats=5, random_state=0, n_jobs=-1)\n",
    "print(\"importance calculation time: {:.2f} s\".format(time.time() - ts))\n",
    "# save importances to DataFrame\n",
    "importances['permutation'] = result.importances_mean\n",
    "importances['std'] = result.importances_std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2RRKPPdXG54N"
   },
   "outputs": [],
   "source": [
    "# plot permutation importances\n",
    "fig, ax = plt.subplots()\n",
    "sns.barplot(y='param', x='permutation', data=importances, ax=ax, hue='param', xerr='std')\n",
    "ax.set_ylabel('')\n",
    "ax.set_xlabel('Permutation feature importance')\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tdyXby7BG54N"
   },
   "source": [
    "## Comparison of feature importances\n",
    "\n",
    "After investigating the biserial correlation, the Gini importance and the permutation importance, the obtained results are now compared."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JzC7GKFKG54O"
   },
   "source": [
    "The following table highlights the 5 highest absolute values in each of the importance measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WHgJi7LYG54O"
   },
   "outputs": [],
   "source": [
    "#Drop the stds, we don't need it anymore\n",
    "importances.drop(columns = 'std')\n",
    "\n",
    "# add biserial correlation\n",
    "importances[\"biserial\"] = correlation_df[\"Correlation\"].values\n",
    "\n",
    "# highlight 5 highest importances in each column\n",
    "styled_importances = (importances\n",
    "                      .style\n",
    "                      .apply(lambda col: ['background-color: skyblue' if (col.dtype==float and abs(val) in col.abs().nlargest(5).values) else '' for val in col],\n",
    "                             axis=0))\n",
    "\n",
    "# Visualizzare il DataFrame con lo stile applicato\n",
    "styled_importances\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "weAW3C-aG54P"
   },
   "source": [
    "In order to compare the importances more easily, each importance measure is normalized so that the absolute values summed over all features add up to one. The Gini importance is normalized this way per default; permutation importance and biserial correlation were adjusted accordingly, even if biserial correlation is not proper measure for compare features, but it´s useful for single feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EBg8ps-5G54P"
   },
   "outputs": [],
   "source": [
    "# plotting the normalized importances\n",
    "bar_width = 0.3\n",
    "fig, ax = plt.subplots(figsize=(7, 7))\n",
    "\n",
    "index = np.arange(len(correlation_df))\n",
    "\n",
    "ax.barh(index, importances['gini'], color='skyblue', height=bar_width, label='Gini importances',edgecolor='none')\n",
    "ax.barh(index + bar_width, np.abs(importances['permutation'])/np.abs(importances['permutation']).sum(), color='green', height=bar_width, label='Permutation importances normalized',edgecolor='none')\n",
    "ax.barh(index+2*bar_width, np.abs(correlation_df.iloc[:,0])/np.abs(correlation_df.iloc[:,0]).sum(), color='orange', height=bar_width, label='Abs of Biserial Correlation normalized',edgecolor='none')\n",
    "\n",
    "ax.set_yticks(index+5/2*bar_width)\n",
    "ax.set_yticklabels(correlation_df.index, va='top')\n",
    "\n",
    "ax.set_ylabel('Feature')\n",
    "ax.set_title('Comparison of Feature Importance Metrics')\n",
    "\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the three metrics are used the permutation one shows a more marked separation in the importance of the features: the m1rem is the most pronounced feature; the other metrics do not emphasize this but they not totally disagree: looking at the table above (and the figure above) we see parameters are highlighted at least twice, if not they are not however the most important of the metric. We can anyway consider the permutation the most reliable metric because it´s the one which randomizes more data, it´s not biased (unlike the Gini importance)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduced Data Set\n",
    "\n",
    "The importance assessment is repeated with a reduced data set. This data set contains the parameters that were suggested to be important and marked as bold in the project assignment. The methods are exactly the same as the ones used for the full data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_reduced = data[[\"m1ZAMS\", \"m2ZAMS\", \"m1rem\", \"m2rem\", \"delay_time\", \"sma\", \"ecc\", \"alpha\", \"Z\"]]\n",
    "data_reduced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the reduced data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_reduced\n",
    "y = data[\"CE\"]\n",
    "\n",
    "# split data into training and test set (10% to 90%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.9, random_state=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialising and training the Random Forest model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise random forest classifier\n",
    "# using n_jobs=-1 to run parallel jobs using all processors\n",
    "clf = RandomForestClassifier(random_state=0, n_jobs=-1)\n",
    "\n",
    "# train random forest\n",
    "# time the process\n",
    "ts = time.time()\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"Training time: %.2f s\"%(time.time()-ts))\n",
    "\n",
    "# evaluate training & test accuracy\n",
    "train_acc = clf.score(X_train, y_train)\n",
    "test_acc = clf.score(X_test, y_test)\n",
    "print(\"Training accuracy: \", train_acc)\n",
    "print(\"Test accuracy: \", test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assessing the Gini and permutation importances:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save params + importances to dataframe\n",
    "importances_reduced = pd.DataFrame(zip(clf.feature_names_in_, clf.feature_importances_),\n",
    "                           columns=[\"param\", \"gini\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot Gini importances\n",
    "fig, ax = plt.subplots()\n",
    "sns.barplot(y='param', x='gini', data=importances_reduced, ax=ax, hue='param')\n",
    "ax.set_ylabel('')\n",
    "ax.set_xlabel('Gini importance')\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute permutation feature importances\n",
    "# warning: takes a lot of time to compute!\n",
    "ts = time.time()\n",
    "result = permutation_importance(clf, X_test[::9], y_test[::9], n_repeats=5, random_state=0, n_jobs=-1)\n",
    "print(\"importance calculation time: {:.2f} s\".format(time.time() - ts))\n",
    "# save importances to DataFrame      \n",
    "importances_reduced['permutation'] = result.importances_mean\n",
    "importances_reduced['permutation_std'] = result.importances_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot permutation importances\n",
    "fig, ax = plt.subplots()\n",
    "sns.barplot(y='param', x='permutation', data=importances_reduced, ax=ax, hue='param')\n",
    "ax.set_ylabel('')\n",
    "ax.set_xlabel('Permutation feature importance')\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the reduced data set, the two importance metrics assign high importances to the mass parameters, with m2ZAMS being deemed the most important. They disagree about the importance of the semimajor axis (sma) and the eccentricity (ecc), but assign low importances to the metallicity Z and the delay time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_red=correlation_df.T[[\"m1ZAMS\", \"m2ZAMS\", \"m1rem\", \"m2rem\", \"delay_time\", \"sma\", \"ecc\", \"alpha\", \"Z\"]].T\n",
    "corr_red\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We show again this graph to compare it with the next one\n",
    "bar_width = 0.3\n",
    "fig, ax = plt.subplots(2,1,figsize=(7, 12))\n",
    "\n",
    "index = np.arange(len(correlation_df))\n",
    "\n",
    "ax[0].barh(index, importances['gini'], color='skyblue', height=bar_width, label='Gini importances',edgecolor='none')\n",
    "ax[0].barh(index + bar_width, np.abs(importances['permutation'])/np.abs(importances['permutation']).sum(), color='green', height=bar_width, label='Permutation importances normalized',edgecolor='none')\n",
    "ax[0].barh(index+2*bar_width, np.abs(correlation_df.iloc[:,0])/np.abs(correlation_df.iloc[:,0]).sum(), color='orange', height=bar_width, label='Abs of Biserial Correlation normalized',edgecolor='none')\n",
    "\n",
    "ax[0].set_yticks(index+5/2*bar_width)\n",
    "ax[0].set_yticklabels(correlation_df.index, va='top')\n",
    "\n",
    "ax[0].set_ylabel('Feature')\n",
    "ax[0].set_title('Comparison of Feature Importance Metrics')\n",
    "\n",
    "ax[0].legend()\n",
    "\n",
    "index_red = np.arange(len(corr_red))\n",
    "\n",
    "ax[1].barh(index_red, importances_reduced['gini'], color='skyblue', height=bar_width, label='Gini importances',edgecolor='none')\n",
    "ax[1].barh(index_red + bar_width, np.abs(importances_reduced['permutation'])/np.abs(importances_reduced['permutation']).sum(), color='green', height=bar_width, label='Permutation importances normalized',edgecolor='none')\n",
    "ax[1].barh(index_red+2*bar_width, np.abs(corr_red['Correlation'])/np.abs(corr_red['Correlation']).sum(), color='orange', height=bar_width, label='Abs of Biserial Correlation normalized',edgecolor='none')\n",
    "\n",
    "ax[1].set_yticks(index_red+5/2*bar_width)\n",
    "ax[1].set_yticklabels(corr_red.index, va='top')\n",
    "ax[1].set_ylabel('Feature')\n",
    "ax[1].set_title('Comparison of Feature Importance Metrics Using Fewer Features')\n",
    "ax[1].legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "The goal of this project was to understand the importance of the features in distinguishing the evolution of a Binary Black Hole System through Common Envelope versus Mass Transfer. This was done by analyzing their Bisereal Correlation and by applying a Random Forest Classificator that was able to preidct the evolution of the system with a great accuracy (99.84%). After evaluating the Feature Pearmutation IMportance and the Gini Importance, we concluded that the most important features are:\n",
    "-"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
