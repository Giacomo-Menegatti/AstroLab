{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6OC7XZYG53y"
      },
      "source": [
        "# Binary star evolution and binary black holes\n",
        "\n",
        "Giacomo Menegatti, Dario Puggioni, Laura Schulze, Savina Tsichli"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "nl4BczkSG7hF"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NdzvZVN7G535"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "A binary black hole can originate from the evolution of a massive binary star. During its life,\n",
        "a tight massive binary star undergoes several complex physical processes: the two stars can\n",
        "exchange mass either via stable mass transfer or via a common envelope. This has a profound\n",
        "impact on the final masses and orbital properties of the binary black hole.  \n",
        "\n",
        "This project aims to differentiate binary black hole systems that evolved via stable mass transfer from those who evolved via common envelop by investigating simulation data. In each dataset of the simulated data, systems which underwent common envelope are marked with \"True\" and systems which underwent stable mass transfer are marked with \"False\" in the \"CE\" parameter. The data are analyzed to identify the features with the highest impact on the evolution of the binary system regarding mass transfer and common envelope. For this purpose, the biserial correlation as well as the parameter importances in a machine learning model are investigated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "y7eUg2FBG537"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "L9g3VwUgG539"
      },
      "outputs": [],
      "source": [
        "# import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set_style(\"darkgrid\")\n",
        "import regex as re\n",
        "import time\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.inspection import permutation_importance\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ia5goWz4HQJd",
        "outputId": "43ad2744-5c93-4cea-bcb7-cfc90b2d6a90"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from posix import pathconf_names\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "import os\n",
        "path = '/content/gdrive/MyDrive'\n",
        "os.chdir(path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uI-EavVDG54A"
      },
      "source": [
        "## Loading the dataset\n",
        "The dataset is divided in files depending on the CE efficiency $ \\alpha $ and the metallicity $ Z $ of the two stars. All the data are loaded into a pandas dataframe containing also the two parameters. The following table contains the parameter names, units and descriptions.\n",
        "\n",
        "\n",
        "| Column name | Unit | Parameter description |\n",
        "|------|------|------|\n",
        "| m1ZAMS | $M_\\odot$ | zero-age main sequence mass of the primary member of the binary system |\n",
        "| m2ZAMS | $M_\\odot$ | zero-age main sequence mass of the secondary member of the binary system |\n",
        "| m1rem | $M_\\odot$ | mass of the black hole that forms from the primary member |\n",
        "| m2rem | $M_\\odot$ | mass of the black hole that forms from the secondary member |\n",
        "| m1rem+m2rem | $M_\\odot$ |  sum of the masses of the two black holes |\n",
        "| delay_time | Myr | delay time |\n",
        "| sma | $R_\\odot$ | semi-major axis of the binary system at the formation of the second-born black hole |\n",
        "| ecc | - | orbital eccentricity of the binary system at the formation of the second-born black hole | kick_1 | km/a | magnitude of the supernova kick associated with the primary member |\n",
        "| kick_2 | km/s | magnitude of the supernova kick associated with the secondary member |\n",
        "| cmu1 | - | cosine of the tilt angle between the orbital angular momentum of the binary system before and after the supernova explosion of the primary member |\n",
        "| cmu2 | - | cosine of the tilt angle between the orbital angular momentum of the binary system before and after the supernova explosion of the secondary member |\n",
        "| v1x | km/s | x component of the centre-of-mass velocity of the binary system after the supernova explosion of the primary component |\n",
        "| v1y | km/s |  y component of the centre-of-mass velocity of the binary system after the supernova explosion of the primary component |\n",
        "| v1z | km/s | z component of the centre-of-mass velocity of the binary system after the supernova explosion of the primary component |\n",
        "| v2x | km/s | x component of the centre-of-mass velocity of the binary system after the supernova explosion of the secondary component |\n",
        "| v2y | km/s | y component of the centre-of-mass velocity of the binary system after the supernova explosion of the secondary component |\n",
        "| v2z | km/s | z component of the centre-of-mass velocity of the binary system after the supernova explosion of the secondary component |\n",
        "| time_SN1 | Myr | time at which the primary component undergoes a supernova |\n",
        "| time_SN2 | Myr | time at which the secondary component undergoes a supernova |\n",
        "| CE | - | Boolean variable indicating whether the binary system undergoes a common envelope (True) |\n",
        "| alpha | - | efficiency parameter of common envelope |\n",
        "| Z | - | stellar metallicity at which the black holes were produced |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xfd9JZnqG54B"
      },
      "outputs": [],
      "source": [
        "alpha = [0.5, 1, 3, 5] # CE efficiency\n",
        "Z = [2e-4, 4e-4, 8e-4, 1.2e-3, 1.6e-3, 2e-3, 4e-3, 6e-3, 8e-3, 1.2e-2, 1.6e-2, 2e-2]  #Metallicity value\n",
        "\n",
        "data = [] #Data list\n",
        "\n",
        "sim_data = pd.DataFrame({'alpha':[], 'Z':[], 'MtotZAMS':[], 'num_mergers':[]})\n",
        "#Appending all the values in a single big dataframe\n",
        "\n",
        "index = 0\n",
        "for a in alpha:\n",
        "    for m in Z:\n",
        "        # The simulation data first row contains the MtotZAMS and the number of merger for each alpha and metallicity\n",
        "        df = pd.read_csv(f'stable_MT_vs_CE/A{a}/MTCE_BBHs_{m}.txt', sep=' ', nrows = 1, header=0)\n",
        "        sim_data.loc[index] = [a, m, df.iloc[0,0], df.iloc[0,1]]  #Adding the row to the sim_data df\n",
        "        index = index + 1\n",
        "\n",
        "        df = pd.read_csv(f'stable_MT_vs_CE/A{a}/MTCE_BBHs_{m}.txt', header=2, sep= ' ')     # Simulation data\n",
        "        df['alpha'],df['Z'] = a, m                                 #Adding the alpha and Z paramtere in the table\n",
        "\n",
        "        data.append(df)\n",
        "\n",
        "data = pd.concat(data)\n",
        "data.columns = [re.sub('col.*:|/.*$', '', name) for name in data.columns]\n",
        "\n",
        "# separate the kick columns\n",
        "data.columns.values[9] = 'kick_1'\n",
        "data.columns.values[10] = 'kick_2'\n",
        "\n",
        "# switch vx and kick columns\n",
        "data[\"kick_1\"], data[\"v1x\"] =  data[\"v1x\"], data[\"kick_1\"]\n",
        "data[\"kick_2\"], data[\"v2x\"] =  data[\"v2x\"], data[\"kick_2\"]\n",
        "\n",
        "# Drop the ID column\n",
        "data = data.drop(columns=\"ID\")\n",
        "\n",
        "print('Data columns: \\n ', data.columns.values)\n",
        "data.reset_index(drop=True,inplace=True)\n",
        "data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tCBUs7OEG54C"
      },
      "outputs": [],
      "source": [
        "N_data = data.shape[0]\n",
        "\n",
        "CE_data = data.query('CE == True').shape[0]\n",
        "MT_data = data.query('CE == False').shape[0]\n",
        "print(f'There are {CE_data} ({CE_data/N_data * 100:.2f}%) BBHs evolving from common envelope, and {MT_data} ({MT_data/N_data * 100:.2f}%) BBHs evolving from mass transfer, for a total of {N_data} BBHs')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PvTEr8OXG54D"
      },
      "source": [
        "## Investigating biserial correlation of the parameters\n",
        "\n",
        "In this section the point biserial correlation is calculated for each feature with respect to the binary parameter \"CE\". This value can range from -1 to 1 and indicates the grade of a possible correlation between data and a binary condition. A value of zero means there is no correlation, a value of (-)1 means that there is a perfect direct (inverse) correlation. If the data of a feature are distributed equally with respect to the binary condition, the correlation is 0; when data are perfectly separated the value of the biserial correlation is $\\pm 1$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qHO5PqViG54D"
      },
      "outputs": [],
      "source": [
        "numeric_columns = data.drop(columns=\"CE\")\n",
        "correlation_dict = {}\n",
        "for column in numeric_columns:\n",
        "    correlation = data[column].corr(data['CE'])\n",
        "    correlation_dict[column] = correlation\n",
        "correlation_df=pd.DataFrame(correlation_dict,index=['Correlation']).T\n",
        "correlation_df.sort_values(by='Correlation',key=abs,ascending=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbUgJOp0G54F"
      },
      "source": [
        "Histograms of the normalized distribution of the data for each features are shown, plotting them with a different color for the binary condition \"CE\". In order to improve the visualization, for some of these histograms a logarithmic scale is used for the counts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PXTwhwQNG54F"
      },
      "outputs": [],
      "source": [
        "# Function to plot the parameters (only for continue distributions)\n",
        "def plot_continue(ax, feature, uom='', bins = 50, hist_range=None, log=False):\n",
        "    if hist_range == None:\n",
        "        hist_range = (data[feature].min(), data[feature].max())\n",
        "    bin_width = (hist_range[1]-hist_range[0])/bins\n",
        "    data[data.CE == True][feature].hist(bins = bins, range = hist_range, color='blue', alpha=0.5, label='Common Envelope', ax=ax,density=True, log=log)\n",
        "    data[data.CE == False][feature].hist(bins = bins, range = hist_range,color='orange', alpha=0.5, label='Mass Transfer', ax=ax,density=True, log=log)\n",
        "    ax.legend(loc='best')\n",
        "    ax.set_xlabel(f'{feature} [{uom}]')\n",
        "    ax.set_ylabel(f'Relative counts/({bin_width:.3f} {uom})')\n",
        "    ax.set_title(f'{feature}, correlation = {correlation_dict[feature]:.4f}')\n",
        "    return ax\n",
        "\n",
        "def plot_discrete(ax, feature):\n",
        "    # Function to plot a discrete variable\n",
        "\n",
        "    labels_CE, counts_CE = np.unique(data[data['CE'] == True][feature], return_counts= True)\n",
        "    # Making the labels strings so they are equally spaced on the plot\n",
        "    labels_CE = [str(x) for x in labels_CE]\n",
        "\n",
        "    labels_MT, counts_MT = np.unique(data[data['CE'] == False][feature], return_counts= True)\n",
        "    labels_MT = [str(x) for x in labels_MT]\n",
        "    ax.bar(labels_CE, counts_CE/CE_data, alpha=0.5)\n",
        "    ax.bar(labels_MT, counts_MT/MT_data, alpha=0.5)\n",
        "    ax.set_ylabel(f'Relative counts')\n",
        "    ax.set_xlabel(f'{feature}')\n",
        "    ax.set_title(f'{feature}, correlation = {correlation_dict[feature]:.4f}')\n",
        "    return ax\n",
        "\n",
        "\n",
        "fig, axs = plt.subplots(nrows=11, ncols=2, figsize=(12, 40))\n",
        "\n",
        "ax = plot_continue(axs[0,0], 'm1ZAMS', '$M_\\odot$')\n",
        "ax = plot_continue(axs[0,1], 'm2ZAMS', '$M_\\odot$')\n",
        "ax = plot_continue(axs[1,0], 'm1rem', '$M_\\odot$')\n",
        "ax = plot_continue(axs[1,1], 'm2rem', '$M_\\odot$')\n",
        "ax = plot_continue(axs[2,0], 'm1rem+m2rem', '$M_\\odot$')\n",
        "ax = plot_continue(axs[2,1], 'delay_time', 'Myr')\n",
        "ax = plot_continue(axs[3,0], 'sma', '$R_\\odot$', log=True)\n",
        "ax = plot_continue(axs[3,1], 'ecc', '')\n",
        "ax = plot_continue(axs[4,0], 'cmu1', '', log=True)\n",
        "ax = plot_continue(axs[4,1], 'cmu2', '', log=True)\n",
        "ax = plot_continue(axs[5,0], 'kick_1', 'km/s', hist_range=(-1, 20))\n",
        "ax = plot_continue(axs[5,1], 'kick_2', 'km/s', hist_range=(-1,20))\n",
        "ax = plot_continue(axs[6,0], 'v1x', 'km/s', log=True)\n",
        "ax = plot_continue(axs[6,1], 'v2x', 'km/s', log=True)\n",
        "ax = plot_continue(axs[7,0], 'v1y', 'km/s',  log=True)\n",
        "ax = plot_continue(axs[7,1], 'v2y', 'km/s',  log=True)\n",
        "ax = plot_continue(axs[8,0], 'v1z', 'km/s', log=True)\n",
        "ax = plot_continue(axs[8,1], 'v2z', 'km/s', log=True)\n",
        "ax = plot_continue(axs[9,0], 'time_SN1', 'Myr')\n",
        "ax = plot_continue(axs[9,1], 'time_SN2', 'Myr')\n",
        "\n",
        "#Plotting alpha and Z\n",
        "\n",
        "ax = plot_discrete(axs[10,0], 'alpha')\n",
        "ax = plot_discrete(axs[10,1], 'Z')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCR0ErYuG54G"
      },
      "source": [
        "### Analizing the correlation between parameters\n",
        "Only the upper diagonal part is kept to make visualization easier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Btlz3rrMG54H"
      },
      "outputs": [],
      "source": [
        "data_corr = data.drop(columns='CE')\n",
        "corr_matrix = data_corr.corr()\n",
        "\n",
        "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
        "\n",
        "sns.heatmap(np.abs(corr_matrix), mask=mask, cmap='inferno', linecolor='w')#annot = true to display the values\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XSSqPh4HG54H"
      },
      "source": [
        "## Investigating parameter importance through Machine Learning\n",
        "In the following section, the importance of the parameters is examined further by evaluating a machine learning model.\n",
        "First, a Random Forest model is trained on the simulation data to predict whether a system evolves via common envelope or not. Then, the impact of the different parameters on the model and its prediction accuracy is investigated."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4LHDTtyG54I"
      },
      "source": [
        "### Training the Random Forest Model\n",
        "The Random Forest was implemented using the `RandomForestClassifier` from the sklearn.ensemble module. As per default, the Random Forest consists of 100 decision trees. It is trained using a 10%-subset of the simulation data, and the prediction accuracy is tested using the remaining 90% of the data. A test accuracy of approx. 99.8% is achieved."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jvmo8nJoG54I"
      },
      "outputs": [],
      "source": [
        "# prepare data X and labels y\n",
        "X = data.drop(columns='CE')\n",
        "y = data['CE']\n",
        "\n",
        "# split data into training and test set (10% to 90%)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.9, random_state=100)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RX7sS9GCG54J"
      },
      "outputs": [],
      "source": [
        "# initialise random forest classifier\n",
        "# using n_jobs=-1 to run parallel jobs using all processors\n",
        "clf = RandomForestClassifier(random_state=0, n_jobs=-1)\n",
        "\n",
        "# train random forest\n",
        "# time the process\n",
        "ts = time.time()\n",
        "clf.fit(X_train, y_train)\n",
        "print(\"Training time: %.2f s\"%(time.time()-ts))\n",
        "\n",
        "# evaluate training & test accuracy\n",
        "train_acc = clf.score(X_train, y_train)\n",
        "test_acc = clf.score(X_test, y_test)\n",
        "print(\"Training accuracy: \", train_acc)\n",
        "print(\"Test accuracy: \", test_acc)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZHNegeu9G54J"
      },
      "outputs": [],
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "def show_confusion_matrix(validations, predictions,perc=False):\n",
        "\n",
        "    matrix = metrics.confusion_matrix(validations, predictions)\n",
        "    plt.figure(figsize=(6, 5))\n",
        "    M = matrix\n",
        "    f = 'd'\n",
        "    cmap=\"GnBu\"\n",
        "    if perc:\n",
        "        M=M/np.sum(M)\n",
        "        f='.2%'\n",
        "        cmap='Blues'\n",
        "    sns.heatmap(M, annot=True, fmt=f, linecolor='white', linewidths=1, cmap=cmap)\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "show_confusion_matrix(y_test, clf.predict(X_test), perc=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYNHN7neG54K"
      },
      "source": [
        "### Analyzing feature importance\n",
        "After training a model that predicts with a satisfactory accuracy, the importance of the features is evaluated using two different metrics; the Gini importance and the Permutation importance.\n",
        "The Gini importance is based on the decrease in impurity, whereas the permutation importance is based on the mean decrease in accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ekn0mMsKG54K"
      },
      "source": [
        "#### Gini importance\n",
        "The Gini importance of a feature is estimated through the weighted impurity decrease at the nodes where the feature is used to split samples in the decision trees. In scikit-learn, the Gini importances are available through the `feature_importances_`-attribute of the `RandomForestClassifier`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "39WyPGakG54K"
      },
      "outputs": [],
      "source": [
        "# save params + importances to dataframe\n",
        "importances = pd.DataFrame(zip(clf.feature_names_in_, clf.feature_importances_),\n",
        "                           columns=[\"param\", \"gini\"])\n",
        "# plot Gini importances\n",
        "fig, ax = plt.subplots()\n",
        "sns.barplot(y='param', x='gini', data=importances, ax=ax)\n",
        "ax.set_ylabel('')\n",
        "ax.set_xlabel('Gini importance')\n",
        "fig.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKmLPK8JG54K"
      },
      "source": [
        "## Adding noise channels to have a reference of Gini importance\n",
        "\n",
        "In this part of the analysis we add a column of pure noise to have a reference in the plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1_4TWMLbG54L"
      },
      "outputs": [],
      "source": [
        "X_noise = X.copy()\n",
        "X_noise['noise'] = np.random.normal(0, 1, size = N_data)\n",
        "\n",
        "# split data into training and test set (10% to 90%)\n",
        "X_train_noisy, X_test_noisy, y_train, y_test = train_test_split(X_noise, y, stratify=y, test_size=0.9, random_state=100)\n",
        "\n",
        "# initialise random forest classifier\n",
        "# using n_jobs=-1 to run parallel jobs using all processors\n",
        "clf_noisy = RandomForestClassifier(random_state=0, n_jobs=-1)\n",
        "\n",
        "# train random forest\n",
        "# time the process\n",
        "ts = time.time()\n",
        "clf_noisy.fit(X_train_noisy, y_train)\n",
        "print(\"Training time: %.2f s\"%(time.time()-ts))\n",
        "\n",
        "# evaluate training & test accuracy\n",
        "train_acc = clf_noisy.score(X_train_noisy, y_train)\n",
        "test_acc = clf_noisy.score(X_test_noisy, y_test)\n",
        "print(\"Training accuracy: \", train_acc)\n",
        "print(\"Test accuracy: \", test_acc)\n",
        "show_confusion_matrix(y_test, clf_noisy.predict(X_test_noisy), perc=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uJZiCsXiG54L"
      },
      "outputs": [],
      "source": [
        "# save params + importances to dataframe\n",
        "importances_noisy = pd.DataFrame(zip(clf_noisy.feature_names_in_, clf_noisy.feature_importances_),columns=[\"param\", \"gini\"])\n",
        "# plot Gini importances\n",
        "fig, ax = plt.subplots()\n",
        "clrs = ['r' if (param == 'noise') else 'b' for param in clf_noisy.feature_names_in_]\n",
        "sns.barplot(y='param', x='gini', data=importances_noisy, ax=ax)\n",
        "ax.set_ylabel('')\n",
        "ax.set_xlabel('Gini importance with noisy channels')\n",
        "fig.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rAduc-Y5G54M"
      },
      "source": [
        "### WHAT CAN WE SAY ABOUT THE NOISY CLF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYAP0U0aG54M"
      },
      "source": [
        "#### Permutation Feature Importance\n",
        "The permutation importance of a feature describes the mean decrease in accuracy when the values of that feature are randomly permutated, and is computed using out-of-bag samples. Here, the permutation feature importances were calculated with the test data set using the `sklearn.inspection.permutation_importance` method.\n",
        "\n",
        "As the computation of the permutation importance using the entire test set takes a lot of time, a subset corresponding to 10% of the total data was chosen from the test set. Furthermore, the number of repetitions `n_repeats` was set to 5 to limit the computation time as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7KBEw7ZCG54M"
      },
      "outputs": [],
      "source": [
        "# compute permutation feature importances\n",
        "# warning: takes a lot of time to compute!\n",
        "ts = time.time()\n",
        "result = permutation_importance(clf, X_test[::10], y_test[::10], n_repeats=5, random_state=0, n_jobs=-1)\n",
        "print(\"importance calculation time: {:.2f} s\".format(time.time() - ts))\n",
        "# save importances to DataFrame\n",
        "importances['permutation'] = result.importances_mean\n",
        "importances_stds = result.importances_std\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2RRKPPdXG54N"
      },
      "outputs": [],
      "source": [
        "# plot permutation importances\n",
        "fig, ax = plt.subplots()\n",
        "sns.barplot(y='param', x='permutation', data=importances, ax=ax, errorbar=importances)\n",
        "ax.set_ylabel('')\n",
        "ax.set_xlabel('Permutation feature importance')\n",
        "fig.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdyXby7BG54N"
      },
      "source": [
        "## Comparison of feature importances\n",
        "\n",
        "After investigating the biserial correlation, the Gini importance and the permutation importance, the obtained results are now compared."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JzC7GKFKG54O"
      },
      "source": [
        "The following table highlights the 5 highest absolute values in each of the importance measures."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WHgJi7LYG54O"
      },
      "outputs": [],
      "source": [
        "# add biserial correlation\n",
        "importances[\"biserial\"] = correlation_df[\"Correlation\"].values\n",
        "\n",
        "# highlight 5 highest importances in each column\n",
        "styled_importances = (importances\n",
        "                      .style\n",
        "                      .apply(lambda col: ['background-color: skyblue' if (col.dtype==float and abs(val) in col.abs().nlargest(5).values) else '' for val in col],\n",
        "                             axis=0))\n",
        "\n",
        "# Visualizzare il DataFrame con lo stile applicato\n",
        "styled_importances\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "weAW3C-aG54P"
      },
      "source": [
        "In order to compare the importances more easily, each importance measure is normalized so that the absolute values summed over all features add up to one. The Gini importance is normalized this way per default; permutation importance and biserial correlation were adjusted accordingly, even if biserial correlation is not proper measure for compare features, but it´s useful for single feature."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EBg8ps-5G54P"
      },
      "outputs": [],
      "source": [
        "# plotting the normalized importances\n",
        "bar_width = 0.3\n",
        "fig, ax = plt.subplots(figsize=(10, 10))\n",
        "\n",
        "index = np.arange(len(correlation_df))\n",
        "\n",
        "ax.barh(index, importances['gini'], color='skyblue', height=bar_width, label='Gini importances',edgecolor='none')\n",
        "ax.barh(index + bar_width, np.abs(importances['permutation'])/np.abs(importances['permutation']).sum(), color='green', height=bar_width, label='Permutation importances normalized',edgecolor='none')\n",
        "ax.barh(index+2*bar_width, np.abs(correlation_df.iloc[:,0])/np.abs(correlation_df.iloc[:,0]).sum(), color='orange', height=bar_width, label='Abs of Biserial Correlation normalized',edgecolor='none')\n",
        "\n",
        "ax.set_yticks(index+5/2*bar_width)\n",
        "ax.set_yticklabels(correlation_df.index, va='top')\n",
        "\n",
        "ax.set_ylabel('Feature')\n",
        "ax.set_title('Comparison of Feature Importance Metrics')\n",
        "\n",
        "ax.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IN_i9FfCG54Q"
      },
      "source": [
        "In the three metrics are used the permutation one shows a more marked separation in the importance of the features: the m1rem is the most pronounced feature; the other metrics do not emphasize this but they not totally disagree: looking at the table above (and the figure above) we see parameters are highlighted at least twice, if not they are not however the most important of the metric.\n",
        "We can anyway consider the permutation the most reliable metric because it´s the one which randomizes more data, it´s not biased (unlike the Gini importance)."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}